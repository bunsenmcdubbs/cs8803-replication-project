{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Recognition Testing Notebook\n",
    "\n",
    "**Goal:** Locate, extract, and merge all entities of interest from plain text inputs\n",
    "\n",
    "Starting from plain text files in the MPQA dataset, we will use Stanford CoreNLP to find all the relevant entities. Entities will also be merged with several heuristics, including using information from Freebase (Wikidata?) and coreference chains. From there, we can start calculating sentiment and faction relationships between them.\n",
    "\n",
    "This notebook is essentially implementing Section 4.1 (_Document Preprocessing_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll start by setting up the data and the tools. We start by locating all the files we could use in MPQA. Then we setup the `pycorenlp` Python wrapper for Stanford CoreNLP's API (running locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mpqa_dir = '../../data/database.mpqa.3.0'\n",
    "mpqa_doclist_path = os.path.join(mpqa_dir, 'doclist')\n",
    "mpqa_file_paths = [\n",
    "    os.path.join(mpqa_dir, 'docs', path.strip()) \n",
    "    for path in open(mpqa_doclist_path, 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's only use the first file in the list for testing and development. Below are the relative paths to the file and the file contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/database.mpqa.3.0/docs/20010926/23.17.57-23406\n"
     ]
    }
   ],
   "source": [
    "# test_file = mpqa_file_paths[0]\n",
    "test_file = '../../data/database.mpqa.3.0/docs/20010926/23.17.57-23406'\n",
    "print(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      "TAIPEI, Sept 26 (AFP) -- Taiwan President Chen Shui-bian on Wednesday reiterated Taipei's full support for the United States as Washington prepared to launch reprisals against Afghanistan. \n",
      "\t\n",
      "\t\n",
      " \"On behalf of the government and people of the Republic of China (Taiwan's official name), I would like to extend our full support to the George W. Bush administration in its any decision and act against terrorists,\" Chen said while meeting Oregon governor John Kitzhaber. \n",
      "\t\n",
      "\t\n",
      " Taiwan \"would not stand idly by\" because \"the attacks were not only a challenge to the US but also a disruption of peace for mankind,\" Chen said in a statement released by the presidential office. \n",
      "\t\n",
      "\t\n",
      " \"The ROC government will be with the US government firmly.\" \n",
      "\t\n",
      "\t\n",
      " Chen again voiced his condolences to the families of the thousands of Americans killed when hijacked planes plunged into the New York World Trade Center and Pentagon on September 11. \n",
      "\t\n",
      "\t\n",
      " Chen's remarks came as the US was massing forces to launch reprisals against Afghanistan, where the prime suspect Usama bin Laden is believed to be in hiding. \n",
      "\t\n",
      "\t\n",
      " Taiwan's Foreign Minister Tien Hung-mao had vowed to back the US in fighting terrorism saying Taiwan had \"no option\" but to do so, despite a lack of diplomatic ties between Taipei and Washington. \n",
      "\t\n",
      "\t\n",
      " Since his inauguration last year, Bush has backpedalled from his predecessor Bill Clinton's policy of engagement with China. \n",
      "\t\n",
      "\t\n",
      " In April Bush approved the biggest US arms package to Taiwan since 1992. \n",
      "\t\n",
      "\t\n",
      " Washington has remained the biggest arms supplier to Taiwan despite switching diplomatic recognition from Taipei to Beijing in 1979. \n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = open(test_file, 'r').read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "Now that we have our text input, let's feed it into CoreNLP to get the annotations from a parse. We will need named entity recognition (`ner`), and coreference resolution (`coref`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nlp.annotate(text, properties={\n",
    "    'annotators': 'ner,coref',\n",
    "    'outputFormat': 'json'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Entities with Named Entity Recognition (`ner` annotater)\n",
    "\n",
    "The NER parse is stored at the token annotation level under the `ner` key. The tokens are annotated with the entity type or `'O'` (for outside?) in the case of non-entity tokens.\n",
    "\n",
    "Ex) `output['sentences'][0]['tokens'][4]['ner']` will contain the entity type for the fifth token in the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TAIPEI', 'LOCATION')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation = output['sentences'][0]['tokens'][0]\n",
    "(annotation['originalText'], annotation['ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is every (token, entity type) pair in the sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TAIPEI', 'LOCATION'),\n",
       " (',', None),\n",
       " ('Sept', 'DATE'),\n",
       " ('26', 'DATE'),\n",
       " ('(', None),\n",
       " ('AFP', 'ORGANIZATION'),\n",
       " (')', None),\n",
       " ('--', None),\n",
       " ('Taiwan', 'LOCATION'),\n",
       " ('President', None),\n",
       " ('Chen', 'PERSON'),\n",
       " ('Shui-bian', 'PERSON'),\n",
       " ('on', None),\n",
       " ('Wednesday', 'DATE'),\n",
       " ('reiterated', None),\n",
       " ('Taipei', 'LOCATION'),\n",
       " (\"'s\", None),\n",
       " ('full', None),\n",
       " ('support', None),\n",
       " ('for', None),\n",
       " ('the', None),\n",
       " ('United', 'LOCATION'),\n",
       " ('States', 'LOCATION'),\n",
       " ('as', None),\n",
       " ('Washington', 'LOCATION'),\n",
       " ('prepared', None),\n",
       " ('to', None),\n",
       " ('launch', None),\n",
       " ('reprisals', None),\n",
       " ('against', None),\n",
       " ('Afghanistan', 'LOCATION'),\n",
       " ('.', None),\n",
       " ('\"', None),\n",
       " ('On', None),\n",
       " ('behalf', None),\n",
       " ('of', None),\n",
       " ('the', None),\n",
       " ('government', None),\n",
       " ('and', None),\n",
       " ('people', None),\n",
       " ('of', None),\n",
       " ('the', None),\n",
       " ('Republic', 'LOCATION'),\n",
       " ('of', 'LOCATION'),\n",
       " ('China', 'LOCATION'),\n",
       " ('(', None),\n",
       " ('Taiwan', 'LOCATION'),\n",
       " (\"'s\", None),\n",
       " ('official', None),\n",
       " ('name', None),\n",
       " (')', None),\n",
       " (',', None),\n",
       " ('I', None),\n",
       " ('would', None),\n",
       " ('like', None),\n",
       " ('to', None),\n",
       " ('extend', None),\n",
       " ('our', None),\n",
       " ('full', None),\n",
       " ('support', None),\n",
       " ('to', None),\n",
       " ('the', None),\n",
       " ('George', 'PERSON'),\n",
       " ('W.', 'PERSON'),\n",
       " ('Bush', 'PERSON'),\n",
       " ('administration', None),\n",
       " ('in', None),\n",
       " ('its', None),\n",
       " ('any', None),\n",
       " ('decision', None),\n",
       " ('and', None),\n",
       " ('act', None),\n",
       " ('against', None),\n",
       " ('terrorists', None),\n",
       " (',', None),\n",
       " ('\"', None),\n",
       " ('Chen', 'PERSON'),\n",
       " ('said', None),\n",
       " ('while', None),\n",
       " ('meeting', None),\n",
       " ('Oregon', 'LOCATION'),\n",
       " ('governor', None),\n",
       " ('John', 'PERSON'),\n",
       " ('Kitzhaber', 'PERSON'),\n",
       " ('.', None),\n",
       " ('Taiwan', 'LOCATION'),\n",
       " ('\"', None),\n",
       " ('would', None),\n",
       " ('not', None),\n",
       " ('stand', None),\n",
       " ('idly', None),\n",
       " ('by', None),\n",
       " ('\"', None),\n",
       " ('because', None),\n",
       " ('\"', None),\n",
       " ('the', None),\n",
       " ('attacks', None),\n",
       " ('were', None),\n",
       " ('not', None),\n",
       " ('only', None),\n",
       " ('a', None),\n",
       " ('challenge', None),\n",
       " ('to', None),\n",
       " ('the', None),\n",
       " ('US', 'LOCATION'),\n",
       " ('but', None),\n",
       " ('also', None),\n",
       " ('a', None),\n",
       " ('disruption', None),\n",
       " ('of', None),\n",
       " ('peace', None),\n",
       " ('for', None),\n",
       " ('mankind', None),\n",
       " (',', None),\n",
       " ('\"', None),\n",
       " ('Chen', 'PERSON'),\n",
       " ('said', None),\n",
       " ('in', None),\n",
       " ('a', None),\n",
       " ('statement', None),\n",
       " ('released', None),\n",
       " ('by', None),\n",
       " ('the', None),\n",
       " ('presidential', None),\n",
       " ('office', None),\n",
       " ('.', None),\n",
       " ('\"', None),\n",
       " ('The', None),\n",
       " ('ROC', 'LOCATION'),\n",
       " ('government', None),\n",
       " ('will', None),\n",
       " ('be', None),\n",
       " ('with', None),\n",
       " ('the', None),\n",
       " ('US', 'LOCATION'),\n",
       " ('government', None),\n",
       " ('firmly', None),\n",
       " ('.', None),\n",
       " ('\"', None),\n",
       " ('Chen', 'PERSON'),\n",
       " ('again', None),\n",
       " ('voiced', None),\n",
       " ('his', None),\n",
       " ('condolences', None),\n",
       " ('to', None),\n",
       " ('the', None),\n",
       " ('families', None),\n",
       " ('of', None),\n",
       " ('the', None),\n",
       " ('thousands', None),\n",
       " ('of', None),\n",
       " ('Americans', 'MISC'),\n",
       " ('killed', None),\n",
       " ('when', None),\n",
       " ('hijacked', None),\n",
       " ('planes', None),\n",
       " ('plunged', None),\n",
       " ('into', None),\n",
       " ('the', None),\n",
       " ('New', 'LOCATION'),\n",
       " ('York', 'LOCATION'),\n",
       " ('World', 'LOCATION'),\n",
       " ('Trade', 'LOCATION'),\n",
       " ('Center', 'LOCATION'),\n",
       " ('and', None),\n",
       " ('Pentagon', 'ORGANIZATION'),\n",
       " ('on', None),\n",
       " ('September', 'DATE'),\n",
       " ('11', 'DATE'),\n",
       " ('.', None),\n",
       " ('Chen', 'PERSON'),\n",
       " (\"'s\", None),\n",
       " ('remarks', None),\n",
       " ('came', None),\n",
       " ('as', None),\n",
       " ('the', None),\n",
       " ('US', 'LOCATION'),\n",
       " ('was', None),\n",
       " ('massing', None),\n",
       " ('forces', None),\n",
       " ('to', None),\n",
       " ('launch', None),\n",
       " ('reprisals', None),\n",
       " ('against', None),\n",
       " ('Afghanistan', 'LOCATION'),\n",
       " (',', None),\n",
       " ('where', None),\n",
       " ('the', None),\n",
       " ('prime', None),\n",
       " ('suspect', None),\n",
       " ('Usama', 'PERSON'),\n",
       " ('bin', 'PERSON'),\n",
       " ('Laden', 'PERSON'),\n",
       " ('is', None),\n",
       " ('believed', None),\n",
       " ('to', None),\n",
       " ('be', None),\n",
       " ('in', None),\n",
       " ('hiding', None),\n",
       " ('.', None),\n",
       " ('Taiwan', 'LOCATION'),\n",
       " (\"'s\", None),\n",
       " ('Foreign', None),\n",
       " ('Minister', None),\n",
       " ('Tien', 'PERSON'),\n",
       " ('Hung-mao', 'PERSON'),\n",
       " ('had', None),\n",
       " ('vowed', None),\n",
       " ('to', None),\n",
       " ('back', None),\n",
       " ('the', None),\n",
       " ('US', 'LOCATION'),\n",
       " ('in', None),\n",
       " ('fighting', None),\n",
       " ('terrorism', None),\n",
       " ('saying', None),\n",
       " ('Taiwan', 'LOCATION'),\n",
       " ('had', None),\n",
       " ('\"', None),\n",
       " ('no', None),\n",
       " ('option', None),\n",
       " ('\"', None),\n",
       " ('but', None),\n",
       " ('to', None),\n",
       " ('do', None),\n",
       " ('so', None),\n",
       " (',', None),\n",
       " ('despite', None),\n",
       " ('a', None),\n",
       " ('lack', None),\n",
       " ('of', None),\n",
       " ('diplomatic', None),\n",
       " ('ties', None),\n",
       " ('between', None),\n",
       " ('Taipei', 'LOCATION'),\n",
       " ('and', None),\n",
       " ('Washington', 'LOCATION'),\n",
       " ('.', None),\n",
       " ('Since', None),\n",
       " ('his', None),\n",
       " ('inauguration', None),\n",
       " ('last', 'DATE'),\n",
       " ('year', 'DATE'),\n",
       " (',', None),\n",
       " ('Bush', 'PERSON'),\n",
       " ('has', None),\n",
       " ('backpedalled', None),\n",
       " ('from', None),\n",
       " ('his', None),\n",
       " ('predecessor', None),\n",
       " ('Bill', 'PERSON'),\n",
       " ('Clinton', 'PERSON'),\n",
       " (\"'s\", None),\n",
       " ('policy', None),\n",
       " ('of', None),\n",
       " ('engagement', None),\n",
       " ('with', None),\n",
       " ('China', 'LOCATION'),\n",
       " ('.', None),\n",
       " ('In', None),\n",
       " ('April', 'DATE'),\n",
       " ('Bush', 'PERSON'),\n",
       " ('approved', None),\n",
       " ('the', None),\n",
       " ('biggest', None),\n",
       " ('US', 'LOCATION'),\n",
       " ('arms', None),\n",
       " ('package', None),\n",
       " ('to', None),\n",
       " ('Taiwan', 'LOCATION'),\n",
       " ('since', None),\n",
       " ('1992', 'DATE'),\n",
       " ('.', None),\n",
       " ('Washington', 'LOCATION'),\n",
       " ('has', None),\n",
       " ('remained', None),\n",
       " ('the', None),\n",
       " ('biggest', None),\n",
       " ('arms', None),\n",
       " ('supplier', None),\n",
       " ('to', None),\n",
       " ('Taiwan', 'LOCATION'),\n",
       " ('despite', None),\n",
       " ('switching', None),\n",
       " ('diplomatic', None),\n",
       " ('recognition', None),\n",
       " ('from', None),\n",
       " ('Taipei', 'LOCATION'),\n",
       " ('to', None),\n",
       " ('Beijing', 'LOCATION'),\n",
       " ('in', None),\n",
       " ('1979', 'DATE'),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(\n",
    "    annotated['originalText'], \n",
    "    annotated['ner'] if annotated['ner'] != 'O' else None\n",
    ")\n",
    " for sentence in output['sentences'] \n",
    " for annotated in sentence['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll need to identify all the relevant entities in the annotated text. The original authors of the paper omitted entities of type date, duration, money, time, and number. We include that filter in this step.\n",
    "\n",
    "We will build a mapping of entities to list of occurances in the text `(sentence_index, start_token_index, end_token_index)`. This will initially be matched solely on token matching but then will be extended to include entity merging heuristics mentioned in the paper.\n",
    "\n",
    "Entities can span multiple tokens but never span sentences. Annotation is done on a per-token basis so any adjacent tokens with the same entity type will be merged into a single multi-token entity span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_id = 0\n",
    "text_to_id = {} # this raw text (tuple of N strings) to id mapping will make entity merging easier\n",
    "occurances = {} # entity occurances will be mapped by id\n",
    "\n",
    "def clear_occurances():\n",
    "    global next_id, text_to_id, occurances\n",
    "    next_id = 0\n",
    "    text_to_id = {}\n",
    "    occurances = {}\n",
    "    \n",
    "def create_entity(name, eid):\n",
    "    global text_to_id, occurances\n",
    "    assert name not in text_to_id\n",
    "    text_to_id[name] = eid\n",
    "    occurances[eid] = set() if eid not in occurances else occurances[eid]\n",
    "\n",
    "def add_occurance(entity, sentence_idx, start_token_idx, end_token_idx):\n",
    "    global next_id\n",
    "    if entity not in text_to_id:\n",
    "        create_entity(entity, next_id)\n",
    "        next_id += 1\n",
    "    add_occurances(entity, {(sentence_idx, start_token_idx, end_token_idx)})\n",
    "\n",
    "def add_occurances(entity, occurances_iter):\n",
    "    global next_id\n",
    "    if entity not in text_to_id:\n",
    "        create_entity(entity, next_id)\n",
    "        next_id += 1\n",
    "    occurances[text_to_id[entity]].update(set(occurances_iter))\n",
    "\n",
    "def get_occurances_by_entity(type_entity_pair):\n",
    "    if type_entity_pair not in text_to_id:\n",
    "        return None\n",
    "    return occurances[text_to_id[type_entity_pair]]\n",
    "\n",
    "def get_attr(tokens_slice, attr):\n",
    "    return [token[attr] for token in tokens_slice if attr in token]\n",
    "\n",
    "def get_attr_default(tokens_slice, attr, default_val):\n",
    "    return [token[attr] if attr in token else default_val for token in tokens_slice]\n",
    "\n",
    "def get_text(tokens_slice):\n",
    "    raw_text = []\n",
    "    for token in tokens_slice:\n",
    "        raw_text.append(token['originalText'])\n",
    "        raw_text.append(token['after'])\n",
    "    return ''.join(raw_text[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_occurances()\n",
    "named_types = set(['PERSON', 'LOCATION', 'ORGANIZATION', 'MISC'])\n",
    "for sentence in output['sentences']:\n",
    "    start_idx = None\n",
    "    curr_type = None\n",
    "    for token in sentence['tokens']:\n",
    "#         if token['ner'] != 'O' and token['ner'] in named_types:\n",
    "#             print('\\t', token['originalText'], token['ner'])\n",
    "        if token['ner'] != curr_type and curr_type is not None:\n",
    "            end_idx = token['index'] - 1\n",
    "            raw_text = get_text(sentence['tokens'][start_idx:end_idx])\n",
    "            add_occurance((curr_type, raw_text), sentence['index'], start_idx, end_idx)\n",
    "            start_idx = curr_type = None\n",
    "        if token['ner'] in named_types and curr_type is None:\n",
    "            start_idx = token['index'] - 1\n",
    "            curr_type = token['ner']\n",
    "    if curr_type is not None:\n",
    "        end_idx = token['index'] - 1\n",
    "        raw_text = get_text(sentence['tokens'][start_idx:end_idx])\n",
    "        add_occurance(raw_text, sentence['index'], start_idx, end_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at all the entities that we have found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count, entity\n",
      "1, ('LOCATION', 'TAIPEI'), {(0, 0, 1)}\n",
      "1, ('ORGANIZATION', 'AFP'), {(0, 5, 6)}\n",
      "7, ('LOCATION', 'Taiwan'), {(8, 10, 11), (2, 0, 1), (9, 8, 9), (6, 0, 1), (6, 16, 17), (0, 8, 9), (1, 14, 15)}\n",
      "1, ('PERSON', 'Chen Shui-bian'), {(0, 10, 12)}\n",
      "3, ('LOCATION', 'Taipei'), {(0, 15, 16), (6, 34, 35), (9, 14, 15)}\n",
      "1, ('LOCATION', 'United States'), {(0, 21, 23)}\n",
      "3, ('LOCATION', 'Washington'), {(6, 36, 37), (0, 24, 25), (9, 0, 1)}\n",
      "2, ('LOCATION', 'Afghanistan'), {(0, 30, 31), (5, 14, 15)}\n",
      "1, ('LOCATION', 'Republic of China'), {(1, 10, 13)}\n",
      "1, ('PERSON', 'George W. Bush'), {(1, 30, 33)}\n",
      "4, ('PERSON', 'Chen'), {(4, 0, 1), (2, 30, 31), (5, 0, 1), (1, 44, 45)}\n",
      "1, ('LOCATION', 'Oregon'), {(1, 48, 49)}\n",
      "1, ('PERSON', 'John Kitzhaber'), {(1, 50, 52)}\n",
      "5, ('LOCATION', 'US'), {(8, 6, 7), (5, 6, 7), (6, 11, 12), (3, 8, 9), (2, 19, 20)}\n",
      "1, ('LOCATION', 'ROC'), {(3, 2, 3)}\n",
      "1, ('MISC', 'Americans'), {(4, 12, 13)}\n",
      "1, ('LOCATION', 'New York World Trade Center'), {(4, 20, 25)}\n",
      "1, ('ORGANIZATION', 'Pentagon'), {(4, 26, 27)}\n",
      "1, ('PERSON', 'Usama bin Laden'), {(5, 20, 23)}\n",
      "1, ('PERSON', 'Tien Hung-mao'), {(6, 4, 6)}\n",
      "2, ('PERSON', 'Bush'), {(8, 2, 3), (7, 6, 7)}\n",
      "1, ('PERSON', 'Bill Clinton'), {(7, 12, 14)}\n",
      "1, ('LOCATION', 'China'), {(7, 19, 20)}\n",
      "1, ('LOCATION', 'Beijing'), {(9, 16, 17)}\n"
     ]
    }
   ],
   "source": [
    "print('count, entity')\n",
    "for entity, entity_id in text_to_id.items():\n",
    "    print(', '.join([str(len(occurances[entity_id])), str(entity), str(occurances[entity_id])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 19, 20), (3, 8, 9), (5, 6, 7), (6, 11, 12), (8, 6, 7)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_occurances_by_entity(('LOCATION', 'US'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Mentions with Co-reference Resolution (`coref` annotator)\n",
    "\n",
    "Co-reference chains are not annotated at the token level and are stored at the top-level of the result object under the `corefs` key. This is a dictionary keyed by unique id's for each chain which point to a list of mentions. These mentions then link back to the tokens through the `sentNum` (sentence number - numbers starting at 1), `startIndex` (index starting at 1), and `endIndex` (exclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['85', '22', '23', '71', '88', '89', '43', '76'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chains = output['corefs']\n",
    "chain_ids = chains.keys()\n",
    "chain_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span\t\t| text_from_tokens | mention['text']\n",
      "(1, 30, 33) \t| George W. Bush | George W. Bush\n",
      "(7, 6, 7) \t| Bush | Bush\n",
      "(7, 10, 11) \t| his | his\n",
      "(8, 1, 3) \t| April Bush | April Bush\n"
     ]
    }
   ],
   "source": [
    "print('span\\t\\t| text_from_tokens | mention[\\'text\\']')\n",
    "for mention in chains['85']:\n",
    "    sentIdx = mention['sentNum'] - 1\n",
    "    startIdx = mention['startIndex'] - 1\n",
    "    endIdx = mention['endIndex'] - 1\n",
    "    text_from_tokens = get_text(output['sentences'][sentIdx]['tokens'][startIdx:endIdx])\n",
    "    print((sentIdx, startIdx, endIdx), '\\t|', text_from_tokens, '|', mention['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Extracted Entities\n",
    "\n",
    "The paper's original authors merged named entities with several heuristics. These include:\n",
    "> merging acronyms, merging named entity of person type with the same last name ... names listed as an alias on Freebase ... mentions in a co-reference chain with the named entity\n",
    "\n",
    "### Merging Aliases and Acronyms using Wikidata\n",
    "\n",
    "[Freebase shutdown at the end of August 2016](https://developers.google.com/freebase/). Much of that data was migrated to [Wikidata](https://wikidata.org). We will be using Wikidata's search and lookup API's rather than running a local instance of the Freebase data dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib # needed for `importlib.reload` - used while developing\n",
    "\n",
    "# import our bespoke Wikidata search/retrieval bindings\n",
    "import wikidata # `wikidata.search` and `wikidata.get`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(wikidata);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing out `wikidata.search` and `wikidata.get` with entities found in our sample document.\n",
    "\n",
    "A search for both 'Republic of China' and 'Taiwan' should yield the same entity on Wikidata (allowing us to merge the two entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aliases': ['Republic of China'],\n",
       " 'concepturi': 'http://www.wikidata.org/entity/Q865',\n",
       " 'description': 'state in East Asia',\n",
       " 'id': 'Q865',\n",
       " 'label': 'Taiwan',\n",
       " 'match': {'language': 'en', 'text': 'Republic of China', 'type': 'alias'},\n",
       " 'pageid': 1185,\n",
       " 'repository': '',\n",
       " 'title': 'Q865',\n",
       " 'url': '//www.wikidata.org/wiki/Q865'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result = wikidata.search('Republic of China')\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aliases': {'en': [{'language': 'en', 'value': 'ROC'},\n",
       "   {'language': 'en', 'value': 'Chinese Taipei'},\n",
       "   {'language': 'en', 'value': 'Chunghwa Minkwo'},\n",
       "   {'language': 'en', 'value': 'Chunghwa Minkuo'},\n",
       "   {'language': 'en', 'value': 'Republic of China'},\n",
       "   {'language': 'en', 'value': '🇹🇼'},\n",
       "   {'language': 'en', 'value': 'tw'}]},\n",
       " 'descriptions': {'en': {'language': 'en', 'value': 'state in East Asia'}},\n",
       " 'id': 'Q865',\n",
       " 'labels': {'en': {'language': 'en', 'value': 'Taiwan'}},\n",
       " 'type': 'item'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity = wikidata.get(search_result['id'])\n",
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity['id'] == wikidata.get_id('Taiwan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They match! As long as this keeps working for ~~all~~ most entities with aliases and acronyms, we'll be all set.\n",
    "\n",
    "Now for the actual merging..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate mappings to merge and use Wikidata id's when available\n",
    "old_text_to_id = text_to_id\n",
    "old_occurances = occurances\n",
    "text_to_id = {}\n",
    "occurances = {}\n",
    "\n",
    "for (e_type, text), old_eid in old_text_to_id.items():\n",
    "    w_entity_id = wikidata.get_id(text)\n",
    "    entity_id = ('wikidata', w_entity_id) if entity is not None and 'id' in entity else ('manual', old_eid)\n",
    "    create_entity((e_type, text), entity_id)\n",
    "    add_occurances((e_type, text), old_occurances[old_eid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count, id, entity\n",
      "4, ('wikidata', 'Q1867'), ('LOCATION', 'TAIPEI')\n",
      "1, ('wikidata', 'Q40464'), ('ORGANIZATION', 'AFP')\n",
      "9, ('wikidata', 'Q865'), ('LOCATION', 'Taiwan')\n",
      "1, ('wikidata', 'Q22368'), ('PERSON', 'Chen Shui-bian')\n",
      "4, ('wikidata', 'Q1867'), ('LOCATION', 'Taipei')\n",
      "6, ('wikidata', 'Q30'), ('LOCATION', 'United States')\n",
      "3, ('wikidata', 'Q61'), ('LOCATION', 'Washington')\n",
      "2, ('wikidata', 'Q889'), ('LOCATION', 'Afghanistan')\n",
      "9, ('wikidata', 'Q865'), ('LOCATION', 'Republic of China')\n",
      "1, ('wikidata', 'Q207'), ('PERSON', 'George W. Bush')\n",
      "4, ('wikidata', 'Q804988'), ('PERSON', 'Chen')\n",
      "1, ('wikidata', 'Q824'), ('LOCATION', 'Oregon')\n",
      "1, ('wikidata', 'Q740345'), ('PERSON', 'John Kitzhaber')\n",
      "6, ('wikidata', 'Q30'), ('LOCATION', 'US')\n",
      "9, ('wikidata', 'Q865'), ('LOCATION', 'ROC')\n",
      "1, ('wikidata', 'Q846570'), ('MISC', 'Americans')\n",
      "1, ('wikidata', None), ('LOCATION', 'New York World Trade Center')\n",
      "1, ('wikidata', 'Q127840'), ('ORGANIZATION', 'Pentagon')\n",
      "1, ('wikidata', 'Q1317'), ('PERSON', 'Usama bin Laden')\n",
      "1, ('wikidata', 'Q9317972'), ('PERSON', 'Tien Hung-mao')\n",
      "2, ('wikidata', 'Q42295'), ('PERSON', 'Bush')\n",
      "1, ('wikidata', 'Q1124'), ('PERSON', 'Bill Clinton')\n",
      "1, ('wikidata', 'Q148'), ('LOCATION', 'China')\n",
      "1, ('wikidata', 'Q956'), ('LOCATION', 'Beijing')\n"
     ]
    }
   ],
   "source": [
    "print('count, id, entity')\n",
    "for entity, entity_id in text_to_id.items():\n",
    "    print(', '.join([str(len(occurances[entity_id])), str(entity_id), str(entity)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: make sure that the new mapping has fewer (or the same) number of keys as the old mapping (because we merged 0+ entities together).\n",
    "\n",
    "In this case, we expect to have several fewer entities through merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 20)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_occurances), len(occurances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging last names\n",
    "\n",
    "The paper authors also merged person named entities when there were exact last name matches. Example from the paper:\n",
    "> e.g. Tiger Woods to Woods\n",
    "\n",
    "The direction of merging might seem somewhat ambigious. Is merging in the direction of `person_entity -> last_name` or `last_name -> person_entity`? In either case, we need to take into account what happens when there are multiple people mentioned with the same last name. Only one-to-one mappings will be merged, making the ambiguity irrelevant.\n",
    "\n",
    "**NOTE** Because the parts of a name are not annotated, it is not possible to distinguish the \"family name\" or \"surname\" portion of full names. As a result, the assumptions around last names will fail in the case of cultures where the family name comes first in the full name. In the sample document the Chinese full name \"Chen Shui-bian\" will not be merged with his last name \"Chen\".\n",
    "\n",
    "In our sample document, we will only be merging \"Bush\" and \"George W. Bush\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('PERSON', 'Bush'), ('wikidata', 'Q42295')),\n",
       " (('PERSON', 'George W. Bush'), ('wikidata', 'Q207')),\n",
       " (('PERSON', 'Chen'), ('wikidata', 'Q804988')),\n",
       " (('PERSON', 'Bill Clinton'), ('wikidata', 'Q1124')),\n",
       " (('PERSON', 'Tien Hung-mao'), ('wikidata', 'Q9317972')),\n",
       " (('PERSON', 'John Kitzhaber'), ('wikidata', 'Q740345')),\n",
       " (('PERSON', 'Usama bin Laden'), ('wikidata', 'Q1317')),\n",
       " (('PERSON', 'Chen Shui-bian'), ('wikidata', 'Q22368'))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = list(filter(lambda pair: pair[0][0] == 'PERSON', text_to_id.items()))\n",
    "# sort by last name (last name = last token (tokenized by string split))\n",
    "names.sort(key=lambda pair: (pair[0][1].split(' ')[-1], len(pair[0][1])))\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial test to ensure that conflicting matches are filtered out properly\n",
    "# Expect only (6, 7)\n",
    "# names = [\n",
    "#     (('', 'A'), 0),\n",
    "#     (('', 'X A'), 1),\n",
    "#     (('', 'XX A'), 1),\n",
    "#     (('', 'XXX A'), 1),\n",
    "#     (('', 'B'), 6),\n",
    "#     (('', 'X B'), 7),\n",
    "#     (('', 'XX B'), 7),\n",
    "#     (('', 'C'), 2),\n",
    "#     (('', 'X C'), 1),\n",
    "#     (('', 'XX C'), 4),\n",
    "#     (('', 'XXXX C'), 5),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('wikidata', 'Q42295'), ('wikidata', 'Q207'))}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_merges = set()\n",
    "curr_last_name = None\n",
    "curr_last_name_eid = None\n",
    "\n",
    "for (_, name), eid in names:\n",
    "    if len(name.split(' ')) == 1:\n",
    "        curr_last_name = name\n",
    "        curr_last_name_eid = eid\n",
    "    elif curr_last_name is not None:\n",
    "        if curr_last_name == name.split(' ')[-1]:\n",
    "            potential_merges.add((curr_last_name_eid, eid))\n",
    "            \n",
    "potential_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('wikidata', 'Q42295'), ('wikidata', 'Q207'))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter down to remove conflicting last name -> full name matches\n",
    "from collections import Counter\n",
    "count = Counter([eid for pair in potential_merges for eid in pair])\n",
    "merges = [pair for pair in potential_merges if count[pair[0]] == 1 and count[pair[1]] == 1]\n",
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eid_dest, eid_additional in merges:\n",
    "    keys_to_change = [entity_key for entity_key, eid in text_to_id.items() if eid == eid_additional]\n",
    "    occurances[eid_dest] |= occurances[eid_additional]\n",
    "    for key in keys_to_change:\n",
    "        text_to_id[key] = eid_dest\n",
    "        del occurances[eid_additional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(occurances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully merged \"Bush\" and \"George W. Bush\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking/Annotating Tokens\n",
    "\n",
    "Now that we've extracted the entities, we'll annotate each token where an entity occurs with the appropriate entity id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_tokens(sentences, span, entity_id):\n",
    "    sent_idx, start_idx, end_idx = span\n",
    "    for token_idx in range(start_idx, end_idx):\n",
    "        sentences[sent_idx]['tokens'][token_idx]['entity_id'] = entity_id\n",
    "\n",
    "for entity_id, spans in occurances.items():\n",
    "    for span in spans:\n",
    "        mark_tokens(output['sentences'], span, entity_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('wikidata', 'Q1867'), {(0, 0, 1), (0, 15, 16), (6, 34, 35), (9, 14, 15)})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eid, test_spans = list(occurances.items())[0]\n",
    "test_eid, test_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 15) Taipei ('wikidata', 'Q1867')\n",
      "(0, 0) TAIPEI ('wikidata', 'Q1867')\n",
      "(6, 34) Taipei ('wikidata', 'Q1867')\n",
      "(9, 14) Taipei ('wikidata', 'Q1867')\n"
     ]
    }
   ],
   "source": [
    "for sent_idx, start_idx, end_idx in test_spans:\n",
    "    for idx in range(start_idx, end_idx):\n",
    "        token = output['sentences'][sent_idx]['tokens'][idx]\n",
    "        print((sent_idx, idx), token['originalText'], token['entity_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Entity Mentions with Co-reference chains\n",
    "\n",
    "Now that we have all of our entities identified and merged, we need to include mentions from co-reference chains as occurances of the entity in the text. We will discard chains that do not contain exactly one entity and then add all mentions in the chain as occurances of the entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chains = output['corefs'].values()\n",
    "chains = []\n",
    "for chain in output['corefs'].values():\n",
    "    entities = set()\n",
    "    for mention in chain:\n",
    "        sentIdx = mention['sentNum'] - 1\n",
    "        startIdx = mention['startIndex'] - 1\n",
    "        endIdx = mention['endIndex'] - 1\n",
    "        mention['span'] = (sentIdx, startIdx, endIdx)\n",
    "        tokens = output['sentences'][sentIdx]['tokens'][startIdx:endIdx]\n",
    "        entities |= set(get_attr(output['sentences'][sentIdx]['tokens'][startIdx:endIdx], 'entity_id'))\n",
    "    if len(entities) == 1:\n",
    "        chains.append((list(entities)[0], chain))\n",
    "\n",
    "len(all_chains), len(chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 8 total chains, we are only interested in including mentions from 5 chains that mention exactly one entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity_id, chain in chains:\n",
    "    for mention in chain:\n",
    "        sentIdx = mention['sentNum'] - 1\n",
    "        startIdx = mention['startIndex'] - 1\n",
    "        endIdx = mention['endIndex'] - 1\n",
    "        mark_tokens(output['sentences'], (sentIdx, startIdx, endIdx), entity_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the tokens in each chain and their (new) entity_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George ('wikidata', 'Q42295')\n",
      "W. ('wikidata', 'Q42295')\n",
      "Bush ('wikidata', 'Q42295')\n",
      "Bush ('wikidata', 'Q42295')\n",
      "his ('wikidata', 'Q42295')\n",
      "April ('wikidata', 'Q42295')\n",
      "Bush ('wikidata', 'Q42295')\n",
      "\n",
      "Taiwan ('wikidata', 'Q865')\n",
      "Taiwan ('wikidata', 'Q865')\n",
      "'s ('wikidata', 'Q865')\n",
      "our ('wikidata', 'Q865')\n",
      "Taiwan ('wikidata', 'Q865')\n",
      "Taiwan ('wikidata', 'Q865')\n",
      "'s ('wikidata', 'Q865')\n",
      "Taiwan ('wikidata', 'Q865')\n",
      "\n",
      "the ('wikidata', 'Q30')\n",
      "US ('wikidata', 'Q30')\n",
      "the ('wikidata', 'Q30')\n",
      "US ('wikidata', 'Q30')\n",
      "\n",
      "Washington ('wikidata', 'Q61')\n",
      "Washington ('wikidata', 'Q61')\n",
      "Washington ('wikidata', 'Q61')\n",
      "\n",
      "Taipei ('wikidata', 'Q1867')\n",
      "'s ('wikidata', 'Q1867')\n",
      "Taipei ('wikidata', 'Q1867')\n",
      "Taipei ('wikidata', 'Q1867')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, chain in chains:\n",
    "    for mention in chain:\n",
    "        sentIdx = mention['sentNum'] - 1\n",
    "        startIdx = mention['startIndex'] - 1\n",
    "        endIdx = mention['endIndex'] - 1\n",
    "        for token in output['sentences'][sentIdx]['tokens'][startIdx:endIdx]:\n",
    "            print(token['originalText'], token['entity_id'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In total, we have 10 sentences, 293 tokens, of which 66 are mentions of 19 entities.\n",
    "\n",
    "In the original paper, the authors wanted to limit the number of entities in a document because of the large number of all-pairs that would require annotation (particularly costly because they used manual annotation in order to create an evaluation dataset). Specifically, they only used documents from the MPQA dataset when they had fewer than 15 entities and only used the first 15 sentences for annotation.\n",
    "\n",
    "(As a result) Our sample MPQA document (`database.mpqa.3.0/docs/20010926/23.17.57-23406`) with 19 entities is not used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = [token for sentence in output['sentences'] for token in sentence['tokens']]\n",
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_occurances = list(filter(lambda token: 'entity_id' in token, all_tokens))\n",
    "len(entity_occurances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_ids = set(map(lambda token: token['entity_id'], entity_occurances))\n",
    "len(entity_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1) TAIPEI ('wikidata', 'Q1867')\n",
      "(0, 6) AFP ('wikidata', 'Q40464')\n",
      "(0, 9) Taiwan ('wikidata', 'Q865')\n",
      "(0, 11) Chen ('wikidata', 'Q22368')\n",
      "(0, 12) Shui-bian ('wikidata', 'Q22368')\n",
      "(0, 16) Taipei ('wikidata', 'Q1867')\n",
      "(0, 17) 's ('wikidata', 'Q1867')\n",
      "(0, 22) United ('wikidata', 'Q30')\n",
      "(0, 23) States ('wikidata', 'Q30')\n",
      "(0, 25) Washington ('wikidata', 'Q61')\n",
      "(0, 31) Afghanistan ('wikidata', 'Q889')\n",
      "(1, 11) Republic ('wikidata', 'Q865')\n",
      "(1, 12) of ('wikidata', 'Q865')\n",
      "(1, 13) China ('wikidata', 'Q865')\n",
      "(1, 15) Taiwan ('wikidata', 'Q865')\n",
      "(1, 16) 's ('wikidata', 'Q865')\n",
      "(1, 26) our ('wikidata', 'Q865')\n",
      "(1, 31) George ('wikidata', 'Q42295')\n",
      "(1, 32) W. ('wikidata', 'Q42295')\n",
      "(1, 33) Bush ('wikidata', 'Q42295')\n",
      "(1, 45) Chen ('wikidata', 'Q804988')\n",
      "(1, 49) Oregon ('wikidata', 'Q824')\n",
      "(1, 51) John ('wikidata', 'Q740345')\n",
      "(1, 52) Kitzhaber ('wikidata', 'Q740345')\n",
      "(2, 1) Taiwan ('wikidata', 'Q865')\n",
      "(2, 20) US ('wikidata', 'Q30')\n",
      "(2, 31) Chen ('wikidata', 'Q804988')\n",
      "(3, 3) ROC ('wikidata', 'Q865')\n",
      "(3, 9) US ('wikidata', 'Q30')\n",
      "(4, 1) Chen ('wikidata', 'Q804988')\n",
      "(4, 13) Americans ('wikidata', 'Q846570')\n",
      "(4, 21) New ('wikidata', None)\n",
      "(4, 22) York ('wikidata', None)\n",
      "(4, 23) World ('wikidata', None)\n",
      "(4, 24) Trade ('wikidata', None)\n",
      "(4, 25) Center ('wikidata', None)\n",
      "(4, 27) Pentagon ('wikidata', 'Q127840')\n",
      "(5, 1) Chen ('wikidata', 'Q804988')\n",
      "(5, 6) the ('wikidata', 'Q30')\n",
      "(5, 7) US ('wikidata', 'Q30')\n",
      "(5, 15) Afghanistan ('wikidata', 'Q889')\n",
      "(5, 21) Usama ('wikidata', 'Q1317')\n",
      "(5, 22) bin ('wikidata', 'Q1317')\n",
      "(5, 23) Laden ('wikidata', 'Q1317')\n",
      "(6, 1) Taiwan ('wikidata', 'Q865')\n",
      "(6, 2) 's ('wikidata', 'Q865')\n",
      "(6, 5) Tien ('wikidata', 'Q9317972')\n",
      "(6, 6) Hung-mao ('wikidata', 'Q9317972')\n",
      "(6, 11) the ('wikidata', 'Q30')\n",
      "(6, 12) US ('wikidata', 'Q30')\n",
      "(6, 17) Taiwan ('wikidata', 'Q865')\n",
      "(6, 35) Taipei ('wikidata', 'Q1867')\n",
      "(6, 37) Washington ('wikidata', 'Q61')\n",
      "(7, 7) Bush ('wikidata', 'Q42295')\n",
      "(7, 11) his ('wikidata', 'Q42295')\n",
      "(7, 13) Bill ('wikidata', 'Q1124')\n",
      "(7, 14) Clinton ('wikidata', 'Q1124')\n",
      "(7, 20) China ('wikidata', 'Q148')\n",
      "(8, 2) April ('wikidata', 'Q42295')\n",
      "(8, 3) Bush ('wikidata', 'Q42295')\n",
      "(8, 7) US ('wikidata', 'Q30')\n",
      "(8, 11) Taiwan ('wikidata', 'Q865')\n",
      "(9, 1) Washington ('wikidata', 'Q61')\n",
      "(9, 9) Taiwan ('wikidata', 'Q865')\n",
      "(9, 15) Taipei ('wikidata', 'Q1867')\n",
      "(9, 17) Beijing ('wikidata', 'Q956')\n"
     ]
    }
   ],
   "source": [
    "for sentence in output['sentences']:\n",
    "    for token in sentence['tokens']:\n",
    "        if 'entity_id' in token:\n",
    "            print((sentence['index'], token['index']), token['originalText'], token['entity_id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
